{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Skenario 1 - CNN VGG16**\n",
    "\n",
    "Skenario 1 menggunakan model Convolutional Neural Network (CNN) dengan arsitektur VGG16 untuk melakukan klasifikasi pada data citra otak.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import packages\n",
    "\n",
    "Impor semua pustaka dan modul yang dibutuhkan untuk pemrosesan data, visualisasi, pembuatan model, dan pelatihan model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import pickle\n",
    "\n",
    "# Scikit-learn imports\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# TensorFlow and Keras imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load dan Eksplorasi Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dan proses data `image_selected.npz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data():\n",
    "    base_dir = r\"D:\\Users\\RESA\\Coding\\Alzheimer-Classification-with-CNN-SVM\\Notebook\\Preprocessing\\image_selected.npz\"\n",
    "    \n",
    "    # Load data dari npz file\n",
    "    loaded_data = np.load(base_dir, allow_pickle=True)\n",
    "    loaded_combined_slices = loaded_data[list(loaded_data.keys())[0]]\n",
    "\n",
    "    # List kosong u/ simpan gmbr n label\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    # Pengganti label kelas AD, CN, EMCI, LMCI\n",
    "    label_mapping = {'AD': 0, 'CN': 1, 'EMCI': 2, 'LMCI': 3}\n",
    "\n",
    "    # Iterate data\n",
    "    for label, plane_slices in loaded_combined_slices.item().items():\n",
    "        for plane, slices in plane_slices.items():\n",
    "            for selected_slice in slices:\n",
    "                position, resized_slice = selected_slice\n",
    "\n",
    "                # Rubah ke format RGB tiap gmbar yang dimuat\n",
    "                resized_slice_rgb = cv2.cvtColor(resized_slice, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "                # Data yg udah diproses ditambah di list X Y\n",
    "                X.append(resized_slice_rgb)\n",
    "                Y.append(label_mapping[label])\n",
    "\n",
    "    # Convert lists ke numpy arrays\n",
    "    X = np.array(X) / 255.0  # Normalisasi\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    # Convert labels to categorical format\n",
    "    Y = to_categorical(Y, num_classes=len(set(Y)))\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Panggil fungsi dan simpan ke X, Y\n",
    "X, Y = load_and_preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (7200, 224, 224, 3)\n",
      "Y shape: (7200, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"X shape:\", X.shape)\n",
    "print(\"Y shape:\", Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribusi Kelas setelah One-Hot Encoding:\n",
      "{0: 1800, 1: 1800, 2: 1800, 3: 1800}\n"
     ]
    }
   ],
   "source": [
    "unique_labels, counts = np.unique(np.argmax(Y, axis=1), return_counts=True)\n",
    "class_distribution = dict(zip(tuple(unique_labels), counts))\n",
    "\n",
    "print(\"Distribusi Kelas setelah One-Hot Encoding:\")\n",
    "print(class_distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Pembagian Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagi dataset\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensi X_train: (5760, 224, 224, 3)\n",
      "Dimensi Y_train: (5760, 4)\n",
      "Dimensi X_test: (1440, 224, 224, 3)\n",
      "Dimensi Y_test: (1440, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"Dimensi X_train:\", X_train.shape)\n",
    "print(\"Dimensi Y_train:\", Y_train.shape)\n",
    "print(\"Dimensi X_test:\", X_test.shape)\n",
    "print(\"Dimensi Y_test:\", Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah sampel dalam X_train: 5760\n",
      "Jumlah sampel dalam Y_train: 5760\n",
      "Jumlah sampel dalam X_test: 1440\n",
      "Jumlah sampel dalam Y_test: 1440\n"
     ]
    }
   ],
   "source": [
    "print(\"Jumlah sampel dalam X_train:\", len(X_train))\n",
    "print(\"Jumlah sampel dalam Y_train:\", len(Y_train))\n",
    "print(\"Jumlah sampel dalam X_test:\", len(X_test))\n",
    "print(\"Jumlah sampel dalam Y_test:\", len(Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Buat dan Konfigurasi Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arsitektur yang digunakan yaitu VGG16 sebagai bagian dari transfer learning yaitu arsitektur yang sudah dilatih dengan data ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "vgg16 (Functional)           (None, 7, 7, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4)                 2052      \n",
      "=================================================================\n",
      "Total params: 14,716,740\n",
      "Trainable params: 2,052\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n",
      "CPU times: total: 1.44 s\n",
      "Wall time: 6.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "base_model = VGG16(\n",
    "    weights='imagenet',\n",
    "    input_shape=(224, 224, 3),\n",
    "    include_top=False\n",
    ")\n",
    "\n",
    "base_model.trainable = False\n",
    "\n",
    "# Membuat inputan layer\n",
    "inputs = Input(shape=(224, 224, 3))\n",
    "\n",
    "# Menggunakan base_model sebagai lapisan dalam model Anda\n",
    "x = base_model(inputs, training=False)\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "outputs = tf.keras.layers.Dense(4, activation='sigmoid')(x)\n",
    "\n",
    "# Menggabungkan input dan output untuk membuat model\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Model\n",
    "Latih model dengan data `X_train dan Y_train` dan menguji kinerja model pada data validasi `X_test dan Y_test`, setelah pelatihan nanti model akan menyimpan riwayatnya ke dalam `h_model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "180/180 [==============================] - 999s 6s/step - loss: 1.3799 - accuracy: 0.2943 - val_loss: 1.3547 - val_accuracy: 0.3326\n",
      "Epoch 2/25\n",
      "180/180 [==============================] - 1089s 6s/step - loss: 1.3258 - accuracy: 0.3737 - val_loss: 1.3241 - val_accuracy: 0.3875\n",
      "Epoch 3/25\n",
      "180/180 [==============================] - 1307s 7s/step - loss: 1.2946 - accuracy: 0.4033 - val_loss: 1.3037 - val_accuracy: 0.4111\n",
      "Epoch 4/25\n",
      "180/180 [==============================] - 1492s 8s/step - loss: 1.2828 - accuracy: 0.4122 - val_loss: 1.2997 - val_accuracy: 0.4160\n",
      "Epoch 5/25\n",
      "180/180 [==============================] - 1503s 8s/step - loss: 1.2619 - accuracy: 0.4459 - val_loss: 1.2865 - val_accuracy: 0.4264\n",
      "Epoch 6/25\n",
      "180/180 [==============================] - 1500s 8s/step - loss: 1.2417 - accuracy: 0.4584 - val_loss: 1.2747 - val_accuracy: 0.4167\n",
      "Epoch 7/25\n",
      "180/180 [==============================] - 1497s 8s/step - loss: 1.2298 - accuracy: 0.4640 - val_loss: 1.2598 - val_accuracy: 0.4278\n",
      "Epoch 8/25\n",
      "180/180 [==============================] - 1271s 7s/step - loss: 1.2110 - accuracy: 0.4812 - val_loss: 1.2481 - val_accuracy: 0.4431\n",
      "Epoch 9/25\n",
      "180/180 [==============================] - 1158s 6s/step - loss: 1.2020 - accuracy: 0.4964 - val_loss: 1.2434 - val_accuracy: 0.4347\n",
      "Epoch 10/25\n",
      "180/180 [==============================] - 1299s 7s/step - loss: 1.1943 - accuracy: 0.4945 - val_loss: 1.2360 - val_accuracy: 0.4389\n",
      "Epoch 11/25\n",
      "180/180 [==============================] - 1544s 9s/step - loss: 1.1907 - accuracy: 0.4976 - val_loss: 1.2317 - val_accuracy: 0.4549\n",
      "Epoch 12/25\n",
      "180/180 [==============================] - 1526s 8s/step - loss: 1.1776 - accuracy: 0.5134 - val_loss: 1.2255 - val_accuracy: 0.4625\n",
      "Epoch 13/25\n",
      "180/180 [==============================] - 1511s 8s/step - loss: 1.1727 - accuracy: 0.5224 - val_loss: 1.2159 - val_accuracy: 0.4639\n",
      "Epoch 14/25\n",
      "180/180 [==============================] - 1473s 8s/step - loss: 1.1647 - accuracy: 0.5167 - val_loss: 1.2085 - val_accuracy: 0.4722\n",
      "Epoch 15/25\n",
      "180/180 [==============================] - 1492s 8s/step - loss: 1.1531 - accuracy: 0.5284 - val_loss: 1.2022 - val_accuracy: 0.4708\n",
      "Epoch 16/25\n",
      "180/180 [==============================] - 1520s 8s/step - loss: 1.1533 - accuracy: 0.5305 - val_loss: 1.1998 - val_accuracy: 0.4896\n",
      "Epoch 17/25\n",
      "180/180 [==============================] - 1337s 7s/step - loss: 1.1449 - accuracy: 0.5311 - val_loss: 1.1934 - val_accuracy: 0.4868\n",
      "Epoch 18/25\n",
      "180/180 [==============================] - 1276s 7s/step - loss: 1.1306 - accuracy: 0.5374 - val_loss: 1.1913 - val_accuracy: 0.4729\n",
      "Epoch 19/25\n",
      "180/180 [==============================] - 1270s 7s/step - loss: 1.1297 - accuracy: 0.5417 - val_loss: 1.1828 - val_accuracy: 0.4882\n",
      "Epoch 20/25\n",
      "180/180 [==============================] - 1443s 8s/step - loss: 1.1191 - accuracy: 0.5464 - val_loss: 1.1833 - val_accuracy: 0.4875\n",
      "Epoch 21/25\n",
      "180/180 [==============================] - 1434s 8s/step - loss: 1.1228 - accuracy: 0.5430 - val_loss: 1.1809 - val_accuracy: 0.4847\n",
      "Epoch 22/25\n",
      "180/180 [==============================] - 1451s 8s/step - loss: 1.1061 - accuracy: 0.5543 - val_loss: 1.1822 - val_accuracy: 0.4854\n",
      "Epoch 23/25\n",
      "180/180 [==============================] - 1434s 8s/step - loss: 1.1084 - accuracy: 0.5602 - val_loss: 1.1742 - val_accuracy: 0.4993\n",
      "Epoch 24/25\n",
      " 70/180 [==========>...................] - ETA: 11:49 - loss: 1.0900 - accuracy: 0.5728"
     ]
    }
   ],
   "source": [
    "EPOCH = 25\n",
    "h_model = model.fit(X_train, Y_train, steps_per_epoch=100, epochs=EPOCH, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Menilai model pada data uji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluasi model menggunakan data uji NumPy\n",
    "evaluation_result = model.evaluate(X_test, Y_test, verbose=1)\n",
    "\n",
    "print(\"Loss:\", evaluation_result[0])\n",
    "print(\"Accuracy:\", evaluation_result[1])\n",
    "\n",
    "# Prediksi menggunakan data uji\n",
    "predictions = model.predict(X_test)\n",
    "predicted_labels = tf.argmax(predictions, axis=1).numpy()\n",
    "\n",
    "print('Label Sebenarnya : ', np.argmax(Y_test, axis=1))\n",
    "print('Label Prediksi : ', predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Visualisasi beberapa gambar dari data uji\n",
    "Membuat visualisasi dari beberapa contoh hasil prediksi model pada data test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"AD\", \"CN\", \"EMCI\", \"LMCI\"]\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(min(9, len(X_test))):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(X_test[i].astype(\"uint8\"))\n",
    "    \n",
    "    # Dapatkan label prediksi dan label sebenarnya\n",
    "    true_label = np.argmax(Y_test[i])\n",
    "    plt.title(\"Pred: \" + class_names[predicted_labels[i]] + \" | Real: \" + class_names[true_label])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluasi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Matriks specificity dan sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def specificity(y_true, y_pred):\n",
    "    true_negatives = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n",
    "    possible_negatives = K.sum(K.round(K.clip(1 - y_true, 0, 1)))\n",
    "    return true_negatives / (possible_negatives + K.epsilon())\n",
    "\n",
    "def sensitivity(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    return true_positives / (possible_positives + K.epsilon())\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', specificity, sensitivity])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matriks pada data train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy, specificity, sensitivity = model.evaluate(X_train, Y_train)\n",
    "print('Loss train set: ', loss)\n",
    "print('Akurasi train set: ', accuracy)\n",
    "print('Specificity train set: ', specificity)\n",
    "print('Sensitivity train set: ', sensitivity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matriks pada data test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy, specificity, sensitivity = model.evaluate(X_test, Y_test)\n",
    "print('Loss train set: ', loss)\n",
    "print('Akurasi train set: ', accuracy)\n",
    "print('Specificity train set: ', specificity)\n",
    "print('Sensitivity train set: ', sensitivity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Confusion Matrix dan Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# loss\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(h_model.history['loss'], label='loss')\n",
    "ax.plot(h_model.history['val_loss'], label='val_loss')\n",
    "ax.legend()\n",
    "plt.title('VGG16')\n",
    "plt.show()\n",
    "fig.savefig(r'D:\\Users\\RESA\\Coding\\Evaluasi\\VGG16Loss.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracies\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(h_model.history['accuracy'], label='acc')\n",
    "ax.plot(h_model.history['val_accuracy'], label='val acc')\n",
    "ax.legend()\n",
    "plt.title('VGG16')\n",
    "plt.show()\n",
    "fig.savefig(r'D:\\Users\\RESA\\Coding\\Evaluasi\\VGG16Accuracy.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simpan model\n",
    "model.save(r\"D:\\Users\\RESA\\Coding\\Evaluasi\\VGG16.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediksi model pd X_test\n",
    "y_preds =  model.predict(X_test)\n",
    "y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ambil indeks nilai tertinggi dari tiap pred\n",
    "y_preds_ind = np.argmax(y_preds,axis=1)\n",
    "y_preds_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ambil indeks nilai maks dari tiap label sebenarnya\n",
    "qwe=np.argmax(Y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(qwe,y_preds_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(qwe,y_preds_ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 ROC AUC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(qwe, y_preds_ind, pos_label=2)\n",
    "\n",
    "auc = auc(fpr, tpr)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1],\"--\")\n",
    "\n",
    "plt.plot(fpr, tpr, label='RF (area = {:.3f})'.format(auc), color='orange')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hasil visualisasinya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Visualize_Result(acc,val_acc,loss, val_loss):\n",
    "    fig, (ax1, ax2) = plt.subplots(nrows = 1,\n",
    "                                   ncols = 2,\n",
    "                                   figsize = (15,6))\n",
    "\n",
    "    plot1 = ax1.plot(range(0, len(acc)),\n",
    "                     acc,\n",
    "                     label = 'accuracy')\n",
    "\n",
    "    plot2 = ax1.plot(range(0, len(val_acc)),\n",
    "                     val_acc,\n",
    "                     label = 'val_accuracy')\n",
    "\n",
    "    ax1.set(title = 'Accuracy And Val Accuracy progress',\n",
    "            xlabel = 'epoch',\n",
    "            ylabel = 'accuracy/ validation accuracy')\n",
    "\n",
    "    ax1.legend()\n",
    "\n",
    "    plot3 = ax2.plot(range(0, len(loss)),\n",
    "                     loss,\n",
    "                     label = 'loss')\n",
    "    \n",
    "    plot4 = ax2.plot(range(0, len(val_loss)),\n",
    "                     val_loss,\n",
    "                     label = 'val_loss')\n",
    "    \n",
    "    ax2.set(title = 'Loss And Val loss progress',\n",
    "            xlabel = 'epoch',\n",
    "            ylabel = 'loss/ validation loss')\n",
    "\n",
    "    ax2.legend()\n",
    "\n",
    "    fig.suptitle('Result Of Model', fontsize = 20, fontweight = 'bold')\n",
    "    fig.savefig(r'D:\\Users\\RESA\\Coding\\Evaluasi\\Accuracy_Loss_figure_VGG16.png')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "visualize_result = Visualize_Result(h_model.history['accuracy'],h_model.history['val_accuracy'], h_model.history['loss'], h_model.history['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cross - Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# Set early stopping criteria\n",
    "pat = 5 \n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=pat, verbose=1)\n",
    "\n",
    "# Define the model checkpoint callback\n",
    "model_checkpoint = ModelCheckpoint(r'D:\\Users\\RESA\\Coding\\Evaluasi\\h_VGG16.h5', verbose=1, save_best_only=True)\n",
    "\n",
    "# Define a function to fit the model\n",
    "def fit_and_evaluate(X_train, X_test, Y_train, Y_test, EPOCHS=20, BATCH_SIZE=32, model=model):\n",
    "    results = model.fit(X_train, Y_train, epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "                        callbacks=[early_stopping, model_checkpoint], verbose=1, validation_split=0.2)\n",
    "    print(\"Val Score: \", model.evaluate(X_test, Y_test))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds = 5\n",
    "epochs = 20\n",
    "batch_size = 16\n",
    "\n",
    "# Save the model history after fitting so that we can plot later\n",
    "model_history = []\n",
    "\n",
    "for i in range(n_folds):\n",
    "    print(\"Training on Fold: \", i + 1)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2,\n",
    "                                                                random_state=np.random.randint(1, 1000, 1)[0])\n",
    "    model_history.append(fit_and_evaluate(X_train, X_test, Y_train, Y_test, epochs, batch_size))\n",
    "    print(\"=======\" * 12, end=\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. Evaluasi Cross - Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Accuracies vs Epochs')\n",
    "for i in range(len(model_history)):\n",
    "  plt.plot(model_history[i].history['accuracy'], label=f\"Training Fold {i+1}\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "fig.savefig(r'D:\\Users\\RESA\\Coding\\Evaluasi\\Accuracy_vs_Epoch_VGG16.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.title('Train Accuracy vs Val Accuracy')\n",
    "plt.plot(model_history[0].history['accuracy'], label='Train Accuracy Fold 1', color='black')\n",
    "plt.plot(model_history[0].history['val_accuracy'], label='Val Accuracy Fold 1', color='black', linestyle = \"dashdot\")\n",
    "plt.plot(model_history[1].history['accuracy'], label='Train Accuracy Fold 2', color='red', )\n",
    "plt.plot(model_history[1].history['val_accuracy'], label='Val Accuracy Fold 2', color='red', linestyle = \"dashdot\")\n",
    "plt.plot(model_history[2].history['accuracy'], label='Train Accuracy Fold 3', color='green', )\n",
    "plt.plot(model_history[2].history['val_accuracy'], label='Val Accuracy Fold 3', color='green', linestyle = \"dashdot\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "fig.savefig(r'D:\\Users\\RESA\\Coding\\Evaluasi\\TrainAccuracy_vs_ValAccuracy_VGG16.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
