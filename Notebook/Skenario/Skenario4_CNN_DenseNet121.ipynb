{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Skenario 4 - CNN DenseNet121**\n",
    "\n",
    "Skenario 4 menggunakan model Convolutional Neural Network (CNN) dengan arsitektur DenseNet121 untuk melakukan klasifikasi pada data citra otak.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import packages\n",
    "\n",
    "Import library yang dibutuhkan untuk pemrosesan data, image processing, modelling dan visualisasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import pickle\n",
    "\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load dan Eksplorasi Data\n",
    "cari dan cetak data `selected_image.npz` di dalam struktur direktori dan menampilkan informasi tentang jumlah slice untuk setiap label dan plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(img_size):\n",
    "    base_dir = r\"D:\\Users\\RESA\\Coding\\Alzheimer-Classification-with-CNN-SVM\\Notebook\\Preprocessing\\image_selected.npz\"\n",
    "    \n",
    "    # Load data from npz file\n",
    "    loaded_data = np.load(base_dir, allow_pickle=True)\n",
    "    loaded_combined_slices = loaded_data[list(loaded_data.keys())[0]]\n",
    "\n",
    "    # Prepare X and Y lists\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    # Create a label mapping for your classes\n",
    "    label_mapping = {'AD': 0, 'CN': 1, 'EMCI': 2, 'LMCI': 3}\n",
    "\n",
    "    # Iterate through loaded data\n",
    "    for label, plane_slices in loaded_combined_slices.item().items():\n",
    "        for plane, slices in plane_slices.items():\n",
    "            for selected_slice in slices:\n",
    "                position, resized_slice = selected_slice\n",
    "\n",
    "                # Resize the slice to the specified img_size\n",
    "                img_arr = cv2.resize(resized_slice, (img_size, img_size))\n",
    "\n",
    "                # Append data to X and Y\n",
    "                X.append(img_arr)\n",
    "                Y.append(label_mapping[label])\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    # Perform K-fold split\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # Convert labels to categorical format\n",
    "    Y = to_categorical(Y, num_classes=len(set(Y)))\n",
    "\n",
    "    # Data augmentation using ImageDataGenerator\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=360,\n",
    "        horizontal_flip=True\n",
    "    )\n",
    "\n",
    "    return X, Y, kf, datagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 224\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "data_dictionary = {}\n",
    "\n",
    "X, Y, kf, datagen = load_and_preprocess_data(img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (7200, 224, 224)\n",
      "Y shape: (7200, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"X shape:\", X.shape)\n",
    "print(\"Y shape:\", Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribusi Kelas setelah One-Hot Encoding:\n",
      "{0: 1800, 1: 1800, 2: 1800, 3: 1800}\n"
     ]
    }
   ],
   "source": [
    "unique_labels, counts = np.unique(np.argmax(Y, axis=1), return_counts=True)\n",
    "class_distribution = dict(zip(unique_labels, counts))\n",
    "\n",
    "print(\"Distribusi Kelas setelah One-Hot Encoding:\")\n",
    "print(class_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensi X_train: (5760, 224, 224)\n",
      "Dimensi Y_train: (5760, 4)\n",
      "Dimensi X_test: (1440, 224, 224)\n",
      "Dimensi Y_test: (1440, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"Dimensi X_train:\", X_train.shape)\n",
    "print(\"Dimensi Y_train:\", Y_train.shape)\n",
    "print(\"Dimensi X_test:\", X_test.shape)\n",
    "print(\"Dimensi Y_test:\", Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah sampel dalam X_train: 5760\n",
      "Jumlah sampel dalam Y_train: 5760\n",
      "Jumlah sampel dalam X_test: 1440\n",
      "Jumlah sampel dalam Y_test: 1440\n"
     ]
    }
   ],
   "source": [
    "print(\"Jumlah sampel dalam X_train:\", len(X_train))\n",
    "print(\"Jumlah sampel dalam Y_train:\", len(Y_train))\n",
    "print(\"Jumlah sampel dalam X_test:\", len(X_test))\n",
    "print(\"Jumlah sampel dalam Y_test:\", len(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_densenet121(img_size, num_classes):\n",
    "  base_model = DenseNet121(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_shape=(img_size, img_size, 3),\n",
    "    pooling=None  # Remove the global average pooling here\n",
    "  )\n",
    "\n",
    "  x = base_model.output\n",
    "  x = GlobalAveragePooling2D()(x)\n",
    "  x = Dense(1024, activation='relu')(x)\n",
    "  predictions = Dense(num_classes, activation='softmax')(x)  # Specify the number of classes here\n",
    "\n",
    "  densenet_model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "  for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "  densenet_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "  return densenet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(create_model_fn, X, Y, kf, datagen, batch_size=32, epochs=10):\n",
    "  entire_history = []\n",
    "  best_history = None\n",
    "  best_val_accuracy = -np.inf\n",
    "  best_model = None\n",
    "  best_val_x = None\n",
    "  best_val_y = None\n",
    "\n",
    "  for train_index, val_index in kf.split(X):\n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    Y_train, Y_val = Y[train_index], Y[val_index]\n",
    "\n",
    "    # Convert labels to categorical format\n",
    "    num_classes = np.unique(np.argmax(Y, axis=1)).size\n",
    "    Y_train = to_categorical(Y_train, num_classes=num_classes)\n",
    "    Y_val = to_categorical(Y_val, num_classes=num_classes)\n",
    "\n",
    "    clear_session()\n",
    "\n",
    "    # Create a new instance of the model with reset weights\n",
    "    model = create_model_fn(img_size=224, num_classes=num_classes)\n",
    "\n",
    "    train_generator = datagen.flow(np.expand_dims(X_train, axis=-1), Y_train, batch_size=batch_size)\n",
    "\n",
    "    model_history = model.fit(train_generator, steps_per_epoch=len(X_train) // batch_size, epochs=epochs)\n",
    "\n",
    "    entire_history.append(model_history.history)\n",
    "    current_val_accuracy = model_history.history['accuracy'][-1]\n",
    "\n",
    "    if current_val_accuracy > best_val_accuracy:\n",
    "      best_val_accuracy = current_val_accuracy\n",
    "      best_history = model_history.history\n",
    "      best_model = model\n",
    "      best_val_x = X_val\n",
    "      best_val_y = Y_val\n",
    "\n",
    "    del model\n",
    "    del model_history\n",
    "    del X_train\n",
    "    del X_val\n",
    "    del Y_train\n",
    "    del Y_val\n",
    "    gc.collect()\n",
    "\n",
    "  return best_history, best_model, best_val_x, best_val_y, entire_history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
