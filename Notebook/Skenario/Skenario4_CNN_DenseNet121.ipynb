{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Skenario 4 - CNN DenseNet121**\n",
    "\n",
    "Skenario 4 menggunakan model Convolutional Neural Network (CNN) dengan arsitektur DenseNet121 untuk melakukan klasifikasi pada data citra otak.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import packages\n",
    "\n",
    "Import library yang dibutuhkan untuk pemrosesan data, image processing, modelling dan visualisasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import pickle\n",
    "\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.backend import clear_session\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Load dan Eksplorasi Data\n",
    "cari dan cetak data `selected_image.npz` di dalam struktur direktori dan menampilkan informasi tentang jumlah slice untuk setiap label dan plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(img_size):\n",
    "    base_dir = r\"D:\\Users\\RESA\\Coding\\Alzheimer-Classification-with-CNN-SVM\\Notebook\\Preprocessing\\image_selected.npz\"\n",
    "    \n",
    "    # Load data from npz file\n",
    "    loaded_data = np.load(base_dir, allow_pickle=True)\n",
    "    loaded_combined_slices = loaded_data[list(loaded_data.keys())[0]]\n",
    "\n",
    "    # Prepare X and Y lists\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    # Create a label mapping for your classes\n",
    "    label_mapping = {'AD': 0, 'CN': 1, 'EMCI': 2, 'LMCI': 3}\n",
    "\n",
    "    # Iterate through loaded data\n",
    "    for label, plane_slices in loaded_combined_slices.item().items():\n",
    "        for plane, slices in plane_slices.items():\n",
    "            for selected_slice in slices:\n",
    "                position, resized_slice = selected_slice\n",
    "\n",
    "                # Resize the slice to the specified img_size\n",
    "                img_arr = cv2.resize(resized_slice, (img_size, img_size))\n",
    "\n",
    "                # Append data to X and Y\n",
    "                X.append(img_arr)\n",
    "                Y.append(label_mapping[label])\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    # Perform K-fold split\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # Convert labels to categorical format\n",
    "    Y = to_categorical(Y, num_classes=len(set(Y)))\n",
    "\n",
    "    # Data augmentation using ImageDataGenerator\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=360,\n",
    "        horizontal_flip=True\n",
    "    )\n",
    "\n",
    "    return X, Y, kf, datagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 224\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "data_dictionary = {}\n",
    "\n",
    "X, Y, kf, datagen = load_and_preprocess_data(img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (7200, 224, 224)\n",
      "Y shape: (7200, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"X shape:\", X.shape)\n",
    "print(\"Y shape:\", Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribusi Kelas setelah One-Hot Encoding:\n",
      "{0: 1800, 1: 1800, 2: 1800, 3: 1800}\n"
     ]
    }
   ],
   "source": [
    "unique_labels, counts = np.unique(np.argmax(Y, axis=1), return_counts=True)\n",
    "class_distribution = dict(zip(tuple(unique_labels), counts))\n",
    "\n",
    "print(\"Distribusi Kelas setelah One-Hot Encoding:\")\n",
    "print(class_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensi X_train: (5760, 224, 224)\n",
      "Dimensi Y_train: (5760, 4)\n",
      "Dimensi X_test: (1440, 224, 224)\n",
      "Dimensi Y_test: (1440, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"Dimensi X_train:\", X_train.shape)\n",
    "print(\"Dimensi Y_train:\", Y_train.shape)\n",
    "print(\"Dimensi X_test:\", X_test.shape)\n",
    "print(\"Dimensi Y_test:\", Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rgb = np.expand_dims(X_train, axis=-1) \n",
    "X_test_rgb = np.expand_dims(X_test, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rgb = np.repeat(X_train_rgb, 3, axis=-1)\n",
    "X_test_rgb = np.repeat(X_test_rgb, 3, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensi X_train_rgb: (5760, 224, 224, 3)\n",
      "Dimensi X_test_rgb: (1440, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Dimensi X_train_rgb:\", X_train_rgb.shape)\n",
    "print(\"Dimensi X_test_rgb:\", X_test_rgb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah sampel dalam X_train: 5760\n",
      "Jumlah sampel dalam Y_train: 5760\n",
      "Jumlah sampel dalam X_test: 1440\n",
      "Jumlah sampel dalam Y_test: 1440\n"
     ]
    }
   ],
   "source": [
    "print(\"Jumlah sampel dalam X_train:\", len(X_train))\n",
    "print(\"Jumlah sampel dalam Y_train:\", len(Y_train))\n",
    "print(\"Jumlah sampel dalam X_test:\", len(X_test))\n",
    "print(\"Jumlah sampel dalam Y_test:\", len(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_densenet121(img_size, num_classes):\n",
    "  base_model = DenseNet121(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_shape=(img_size, img_size, 3),\n",
    "    pooling=None  # Remove the global average pooling here\n",
    "  )\n",
    "\n",
    "  x = base_model.output\n",
    "  x = GlobalAveragePooling2D()(x)\n",
    "  x = Dense(1024, activation='relu')(x)\n",
    "  predictions = Dense(num_classes, activation='softmax')(x)  # Specify the number of classes here\n",
    "\n",
    "  densenet_model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "  for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "  opt = tf.keras.optimizers.RMSprop(learning_rate=0.0005)\n",
    "\n",
    "  densenet_model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=opt,\n",
    "                  metrics=[tf.keras.metrics.BinaryAccuracy(),\n",
    "                           tf.keras.metrics.AUC(),\n",
    "                           tf.keras.metrics.Precision(),\n",
    "                           tf.keras.metrics.Recall(),\n",
    "                           tf.keras.metrics.FalsePositives(),\n",
    "                           tf.keras.metrics.FalseNegatives(),\n",
    "                           tf.keras.metrics.TruePositives(),\n",
    "                           tf.keras.metrics.TrueNegatives()])\n",
    "\n",
    "\n",
    "  return densenet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(X, Y, kf, datagen, img_size, num_classes, epochs, batch_size):\n",
    "    history_list = []\n",
    "    \n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X)):\n",
    "        print(f\"Training on fold {fold + 1}...\")\n",
    "        \n",
    "        # Split data into training and validation sets\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        Y_train, Y_val = Y[train_index], Y[val_index]\n",
    "\n",
    "        # Expand dimensions and repeat for RGB\n",
    "        X_train_rgb = np.expand_dims(X_train, axis=-1)\n",
    "        X_val_rgb = np.expand_dims(X_val, axis=-1)\n",
    "        X_train_rgb = np.repeat(X_train_rgb, 3, axis=-1)\n",
    "        X_val_rgb = np.repeat(X_val_rgb, 3, axis=-1)\n",
    "\n",
    "        # Create and compile the model\n",
    "        model = create_densenet121(img_size, num_classes)\n",
    "        \n",
    "        # Train the model\n",
    "        history = model.fit(\n",
    "            datagen.flow(X_train_rgb, Y_train, batch_size=batch_size),\n",
    "            steps_per_epoch=len(X_train) // batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_data=(X_val_rgb, Y_val),\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Evaluate the model on the validation set\n",
    "        val_loss, val_acc = model.evaluate(X_val_rgb, Y_val, verbose=0)\n",
    "        print(f\"Validation Accuracy on Fold {fold + 1}: {val_acc * 100:.2f}%\\n\")\n",
    "\n",
    "        # Save the training history\n",
    "        history_list.append(history.history)\n",
    "        \n",
    "        # Clear the TensorFlow session to avoid model interference\n",
    "        clear_session()\n",
    "        gc.collect()\n",
    "\n",
    "    return history_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m history_list \u001b[38;5;241m=\u001b[39m cross_validation(X, Y, kf, datagen, img_size, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28;43mset\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m), epochs, batch_size)\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "history_list = cross_validation(X, Y, kf, datagen, img_size, len(set(Y)), epochs, batch_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
